"""
Summary Generator for Tennis Discovery Agent

Generates summary pages from practice memos using Gemini AI.
"""

from datetime import datetime, timedelta
from pathlib import Path
from typing import Dict, List, Optional, Any
import json
import yaml
import re
from collections import Counter

from src.models.scene_data import SearchFilters


class SummaryGenerator:
    """ã¾ã¨ã‚ãƒšãƒ¼ã‚¸ç”Ÿæˆã‚¨ãƒ³ã‚¸ãƒ³"""

    def __init__(self, obsidian_manager, gemini_client, github_sync):
        """
        Initialize SummaryGenerator.

        Args:
            obsidian_manager: ObsidianManager instance
            gemini_client: GeminiClient instance
            github_sync: GitHubSync instance
        """
        self.obsidian_manager = obsidian_manager
        self.gemini_client = gemini_client
        self.github_sync = github_sync
        self.vault_path = Path(obsidian_manager.vault_path)

    # ========================================
    # Public Methods
    # ========================================

    async def generate_all_summaries(self) -> bool:
        """
        Generate all 6 summary pages.

        Returns:
            True if successful, False otherwise
        """
        try:
            print("ğŸ“Š ã¾ã¨ã‚ãƒšãƒ¼ã‚¸ç”Ÿæˆã‚’é–‹å§‹ã—ã¾ã™...")

            # 1. ã¾ã¨ã‚_ç·åˆ.md
            await self.generate_summary_overview()

            # 2. ã¾ã¨ã‚_æœ€è¿‘.md
            await self.generate_summary_period("recent")

            # 3. ã¾ã¨ã‚_1ãƒ¶æœˆ.md
            await self.generate_summary_period("month")

            # 4. ã¾ã¨ã‚_ãƒ•ã‚©ã‚¢ãƒãƒ³ãƒ‰.md
            await self.generate_summary_technique("ãƒ•ã‚©ã‚¢ãƒãƒ³ãƒ‰")

            # 5. ã¾ã¨ã‚_ãƒãƒƒã‚¯ãƒãƒ³ãƒ‰.md
            await self.generate_summary_technique("ãƒãƒƒã‚¯ãƒãƒ³ãƒ‰")

            # 6. ã¾ã¨ã‚_ã‚µãƒ¼ãƒ–.md
            await self.generate_summary_technique("ã‚µãƒ¼ãƒ–")

            # GitHub push
            self.github_sync.push_to_github()

            print("âœ… ã¾ã¨ã‚ãƒšãƒ¼ã‚¸ç”Ÿæˆå®Œäº†ï¼")
            return True

        except Exception as e:
            print(f"âŒ ã¾ã¨ã‚ãƒšãƒ¼ã‚¸ç”Ÿæˆã‚¨ãƒ©ãƒ¼: {e}")
            return False

    async def generate_summary_overview(self) -> None:
        """Generate ã¾ã¨ã‚_ç·åˆ.md"""
        from src.ai.summary_prompts import SummaryPrompts

        print("  â†’ ã¾ã¨ã‚_ç·åˆ.md ç”Ÿæˆä¸­...")

        # ãƒ‡ãƒ¼ã‚¿åé›†
        data = self.collect_memos_for_summary(period="recent")

        # AIç”Ÿæˆãƒ—ãƒ­ãƒ³ãƒ—ãƒˆ
        prompt = SummaryPrompts.generate_overview_prompt(data)

        # Gemini APIã§ç”Ÿæˆ
        markdown_content = await self._generate_with_gemini(prompt)

        # ãƒ•ã‚¡ã‚¤ãƒ«ä¿å­˜
        output_path = self.vault_path / "ã¾ã¨ã‚_ç·åˆ.md"
        with open(output_path, 'w', encoding='utf-8') as f:
            f.write(markdown_content)

        print("  âœ… ã¾ã¨ã‚_ç·åˆ.md å®Œæˆ")

    async def generate_summary_period(self, period: str) -> None:
        """
        Generate period-based summary.

        Args:
            period: "recent" or "month"
        """
        from src.ai.summary_prompts import SummaryPrompts

        filename = "ã¾ã¨ã‚_æœ€è¿‘.md" if period == "recent" else "ã¾ã¨ã‚_1ãƒ¶æœˆ.md"
        print(f"  â†’ {filename} ç”Ÿæˆä¸­...")

        # ãƒ‡ãƒ¼ã‚¿åé›†
        data = self.collect_memos_for_summary(period=period)

        # AIç”Ÿæˆãƒ—ãƒ­ãƒ³ãƒ—ãƒˆ
        prompt = SummaryPrompts.generate_period_prompt(data, period)

        # Gemini APIã§ç”Ÿæˆ
        markdown_content = await self._generate_with_gemini(prompt)

        # ãƒ•ã‚¡ã‚¤ãƒ«ä¿å­˜
        output_path = self.vault_path / filename
        with open(output_path, 'w', encoding='utf-8') as f:
            f.write(markdown_content)

        print(f"  âœ… {filename} å®Œæˆ")

    async def generate_summary_technique(self, technique: str) -> None:
        """
        Generate technique-based summary.

        Args:
            technique: "ãƒ•ã‚©ã‚¢ãƒãƒ³ãƒ‰", "ãƒãƒƒã‚¯ãƒãƒ³ãƒ‰", "ã‚µãƒ¼ãƒ–"
        """
        from src.ai.summary_prompts import SummaryPrompts

        filename = f"ã¾ã¨ã‚_{technique}.md"
        print(f"  â†’ {filename} ç”Ÿæˆä¸­...")

        # ãƒ‡ãƒ¼ã‚¿åé›†
        data = self.collect_memos_for_summary(period="all", technique=technique)

        # AIç”Ÿæˆãƒ—ãƒ­ãƒ³ãƒ—ãƒˆ
        prompt = SummaryPrompts.generate_technique_prompt(data, technique)

        # Gemini APIã§ç”Ÿæˆ
        markdown_content = await self._generate_with_gemini(prompt)

        # ãƒ•ã‚¡ã‚¤ãƒ«ä¿å­˜
        output_path = self.vault_path / filename
        with open(output_path, 'w', encoding='utf-8') as f:
            f.write(markdown_content)

        print(f"  âœ… {filename} å®Œæˆ")

    # ========================================
    # Data Collection
    # ========================================

    def collect_memos_for_summary(
        self,
        period: str = "all",
        technique: Optional[str] = None
    ) -> Dict[str, Any]:
        """
        Collect memo data for summary generation.

        Args:
            period: "recent" (2 weeks), "month" (1 month), "all"
            technique: Technique name filter (None = all techniques)

        Returns:
            {
                'memos': [...],          # Raw memo data
                'insights': [...],       # Insight list
                'reflections': [...],    # Reflection list (unresolved only)
                'tags': {...},           # Tag counts
                'date_range': {...},     # Period info
                'trends': {...}          # Trend analysis
            }
        """
        # 1. Calculate period
        if period == "recent":
            start_date = datetime.now() - timedelta(days=14)
        elif period == "month":
            start_date = datetime.now() - timedelta(days=30)
        else:
            start_date = None  # All period

        # 2. Get memos
        if technique:
            # Filter by technique tag
            memos = self.obsidian_manager.search(
                filters=SearchFilters(tags=[technique]),
                limit=None
            )
        else:
            # All memos
            memos = self.obsidian_manager._get_all_memos(force_refresh=True)

        # Filter by date
        if start_date:
            memos = [m for m in memos if self._parse_memo_date(m) >= start_date]

        # 3. Extract data
        insights = []
        reflections = []
        tag_counts = {}

        for memo in memos:
            memo_data = self._extract_memo_data_from_dict(memo)

            insights.extend(memo_data['insights'])

            # Only include unresolved reflections
            unresolved = [r for r in memo_data['reflections'] if r['status'] == 'unresolved']
            reflections.extend(unresolved)

            for tag in memo_data['tags']:
                tag_counts[tag] = tag_counts.get(tag, 0) + 1

        # 4. Analyze trends
        trends = self._analyze_trends(memos, period)

        return {
            'memos': memos,
            'insights': insights,
            'reflections': reflections,
            'tags': tag_counts,
            'date_range': {
                'start': start_date.strftime('%Y-%m-%d') if start_date else 'å…¨æœŸé–“',
                'end': datetime.now().strftime('%Y-%m-%d')
            },
            'trends': trends
        }

    def _extract_memo_data_from_dict(self, memo: Dict[str, Any]) -> Dict[str, Any]:
        """
        Extract data from memo dictionary.

        Args:
            memo: Memo dictionary from ObsidianManager

        Returns:
            Extracted memo data
        """
        # Parse Markdown sections
        content = memo.get('content', '')
        markdown_data = self._parse_markdown_sections(content)

        return {
            'date': memo.get('date'),
            'scene': memo.get('scene'),
            'tags': memo.get('tags', []),
            'insights': markdown_data.get('insights', []),
            'reflections': markdown_data.get('reflections', []),
            'deepening': markdown_data.get('deepening', {})
        }

    def _parse_memo_date(self, memo: Dict[str, Any]) -> datetime:
        """
        Parse date from memo.

        Args:
            memo: Memo dictionary

        Returns:
            Datetime object (or far past date if parsing fails)
        """
        date_str = memo.get('date')
        if not date_str:
            return datetime(1900, 1, 1)

        try:
            return datetime.fromisoformat(date_str)
        except:
            return datetime(1900, 1, 1)

    # ========================================
    # Markdown Parsing
    # ========================================

    def _parse_markdown_sections(self, content: str) -> Dict[str, Any]:
        """
        Parse Markdown sections.

        Args:
            content: Markdown content

        Returns:
            Parsed sections data
        """
        # Extract "## æ°—ã¥ã" section
        insights_section = self._extract_section(content, '## æ°—ã¥ã')

        # Extract "## åçœç‚¹" section
        reflections_section = self._extract_section(content, '## åçœç‚¹')

        # Extract "### æ·±å €ã‚Šæƒ…å ±" section
        deepening_section = self._extract_section(content, '### æ·±å €ã‚Šæƒ…å ±')

        return {
            'insights': self._parse_list_items(insights_section),
            'reflections': self._parse_reflections(reflections_section),
            'deepening': self._parse_deepening(deepening_section)
        }

    def _extract_section(self, content: str, section_name: str) -> str:
        """
        Extract specific section from Markdown.

        Args:
            content: Markdown content
            section_name: Section header (e.g., "## æ°—ã¥ã")

        Returns:
            Section content (empty string if not found)
        """
        if section_name not in content:
            return ""

        start = content.index(section_name)
        # Get until next section (##)
        next_section = content.find('\n##', start + len(section_name))

        if next_section == -1:
            return content[start:]
        else:
            return content[start:next_section]

    def _parse_list_items(self, section: str) -> List[str]:
        """
        Parse list items from section.

        Args:
            section: Markdown section

        Returns:
            List of items
        """
        lines = section.split('\n')
        items = []

        for line in lines:
            line = line.strip()
            if line.startswith('- ') or line.startswith('* '):
                items.append(line[2:].strip())

        return items

    def _parse_reflections(self, section: str) -> List[Dict[str, str]]:
        """
        Parse reflections from section.

        Args:
            section: Markdown section

        Returns:
            List of reflection dictionaries
        """
        items = self._parse_list_items(section)
        reflections = []

        for item in items:
            # "[ ]" = unresolved, "[x]" = resolved
            if item.startswith('[ ]'):
                status = 'unresolved'
                content = item[4:].strip()
            elif item.startswith('[x]'):
                status = 'resolved'
                content = item[4:].strip()
            else:
                status = 'unresolved'
                content = item

            reflections.append({
                'content': content,
                'status': status
            })

        return reflections

    def _parse_deepening(self, section: str) -> Dict[str, str]:
        """
        Parse deepening information.

        Args:
            section: Markdown section

        Returns:
            Deepening data dictionary
        """
        deepening = {}

        # Extract "**å¯¾æ¯”**:", "**å¤‰åŒ–**:", etc.
        patterns = ['å¯¾æ¯”', 'å¤‰åŒ–', 'æ ¹æ‹ ', 'å…·ä½“åŒ–']

        for pattern in patterns:
            marker = f"**{pattern}**:"
            if marker in section:
                start = section.index(marker) + len(marker)
                # Until next pattern or end
                end = len(section)
                for other_pattern in patterns:
                    other_marker = f"**{other_pattern}**:"
                    if other_marker in section[start:]:
                        end = start + section[start:].index(other_marker)
                        break

                value = section[start:end].strip()
                deepening[pattern] = value

        return deepening

    # ========================================
    # Trend Analysis
    # ========================================

    def _analyze_trends(self, memos: List[Dict[str, Any]], period: str) -> Dict[str, Any]:
        """
        Analyze trends from memos.

        Args:
            memos: List of memo dictionaries
            period: Period type

        Returns:
            Trend analysis data
        """
        if not memos:
            return {
                'practice_frequency': "ãƒ‡ãƒ¼ã‚¿ãªã—",
                'most_common_tags': [],
                'keywords': {},
                'patterns': []
            }

        # Practice frequency
        practice_count = len(memos)
        days = 14 if period == "recent" else 30 if period == "month" else 365
        frequency = f"{practice_count}å› / {days}æ—¥"

        # Most common tags
        all_tags = []
        for memo in memos:
            all_tags.extend(memo.get('tags', []))

        tag_counter = Counter(all_tags)
        most_common_tags = tag_counter.most_common(5)

        # Keyword extraction
        all_text = []
        for memo in memos:
            content = memo.get('content', '')
            all_text.append(content)

        combined_text = ' '.join(all_text)
        keywords = self._extract_keywords_with_count(combined_text)

        return {
            'practice_frequency': frequency,
            'most_common_tags': most_common_tags,
            'keywords': keywords,
            'patterns': [f"ã€Œ{k}ã€ãŒ{v}å›" for k, v in keywords.items() if v > 2][:5]
        }

    def _extract_keywords_with_count(self, text: str) -> Dict[str, int]:
        """
        Extract keywords with count from text.

        Args:
            text: Text to analyze

        Returns:
            Dictionary of keyword counts
        """
        # Split by particles
        particles = ['ã¯', 'ãŒ', 'ã‚’', 'ã«', 'ã¸', 'ã¨', 'ã‹ã‚‰', 'ã¾ã§', 'ã‚ˆã‚Š', 'ã§', 'ã®']

        for particle in particles:
            text = text.replace(particle, ' ')

        # Remove symbols
        symbols = ['ã€', 'ã€‚', 'ï¼', 'ï¼Ÿ', 'ï¼ˆ', 'ï¼‰', 'ã€Œ', 'ã€', 'ã€', 'ã€‘', 'ã€', 'ã€']
        for symbol in symbols:
            text = text.replace(symbol, ' ')

        # Split and count
        words = text.split()
        words = [w.strip() for w in words if w.strip() and len(w.strip()) >= 2]

        word_counter = Counter(words)
        # Return top 10
        return dict(word_counter.most_common(10))

    # ========================================
    # AI Generation
    # ========================================

    async def _generate_with_gemini(self, prompt: str) -> str:
        """
        Generate content using Gemini API.

        Args:
            prompt: Generation prompt

        Returns:
            Generated Markdown content
        """
        try:
            # Call Gemini API using the model's generate_content method
            response = self.gemini_client.model.generate_content(prompt)

            # Extract text from response
            content = response.text.strip()

            # Remove code blocks if present
            if content.startswith('```markdown'):
                content = content[11:]
            if content.startswith('```'):
                content = content[3:]
            if content.endswith('```'):
                content = content[:-3]

            return content.strip()

        except Exception as e:
            print(f"Gemini API error: {e}")
            import traceback
            traceback.print_exc()
            return f"# ã‚¨ãƒ©ãƒ¼\n\nã¾ã¨ã‚ãƒšãƒ¼ã‚¸ã®ç”Ÿæˆä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {e}"
